{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "475610d0",
   "metadata": {},
   "source": [
    "# Dynamic Autocomplete System Refactoring\n",
    "\n",
    "This notebook analyzes and refactors the current static technology ecosystem data in the autocomplete system, replacing hardcoded mappings with fully dynamic, data-driven approaches.\n",
    "\n",
    "## Objectives:\n",
    "1. **Remove Static Technology Ecosystems** - Replace hardcoded tech_ecosystems dictionary with database-driven discovery\n",
    "2. **Eliminate Hardcoded Limits** - Remove artificial restrictions and implement adaptive result sizing\n",
    "3. **Create Dynamic Scoring** - Build relevance scoring based on real data patterns\n",
    "4. **Optimize Performance** - Ensure the dynamic system maintains or improves current performance\n",
    "\n",
    "## Current Issues:\n",
    "- Static `tech_ecosystems` dictionary with 250+ hardcoded technology mappings\n",
    "- Fixed `important_tech` list with 35+ predefined technologies\n",
    "- Hardcoded limits that restrict result flexibility\n",
    "- Manual maintenance required for technology updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93627a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported and mock objects initialized\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries and Setup\n",
    "import re\n",
    "import difflib\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any, Set, Tuple\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Mock database collection and vectorizer for analysis\n",
    "# In actual implementation, these would connect to MongoDB and embeddings\n",
    "class MockCollection:\n",
    "    def __init__(self):\n",
    "        self.sample_data = [\n",
    "            {\n",
    "                \"skills\": [\"Python\", \"Django\", \"PostgreSQL\", \"Docker\"],\n",
    "                \"technical_skills\": \"JavaScript, React, Node.js, MongoDB\",\n",
    "                \"experience\": [\n",
    "                    {\n",
    "                        \"title\": \"Python Developer\",\n",
    "                        \"skills\": [\"Flask\", \"FastAPI\", \"Redis\", \"AWS\"],\n",
    "                        \"description\": \"Developed web applications using Python Django framework with PostgreSQL database\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"skills\": \"Java, Spring Boot, MySQL, Jenkins\",\n",
    "                \"technical_skills\": [\"Docker\", \"Kubernetes\", \"Git\", \"Maven\"],\n",
    "                \"experience\": [\n",
    "                    {\n",
    "                        \"title\": \"Full Stack Developer\", \n",
    "                        \"skills\": [\"React\", \"JavaScript\", \"HTML\", \"CSS\"],\n",
    "                        \"description\": \"Built microservices using Java Spring Boot and React frontend\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def aggregate(self, pipeline, **kwargs):\n",
    "        return self.sample_data\n",
    "\n",
    "# Initialize mock objects for analysis\n",
    "collection = MockCollection()\n",
    "print(\"‚úÖ Libraries imported and mock objects initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba9a832",
   "metadata": {},
   "source": [
    "## üìä Current Static Technology Ecosystems Analysis\n",
    "\n",
    "Let's examine the current hardcoded technology ecosystem data that we need to replace with dynamic discovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6c58aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç CURRENT STATIC DATA ANALYSIS:\n",
      "üìà Total Ecosystems: 5\n",
      "üìä Total Unique Technologies in Ecosystems: 79\n",
      "‚≠ê Important Technologies Count: 32\n",
      "\n",
      "üîÑ Cross-Ecosystem Overlaps: 18\n",
      "  ‚Ä¢ python ‚Üî java: ['api', 'kubernetes', 'sql', 'aws', 'postgresql']...\n",
      "  ‚Ä¢ python ‚Üî javascript: ['api', 'mongodb', 'aws', 'postgresql', 'git']...\n",
      "  ‚Ä¢ python ‚Üî react: ['rest', 'api', 'git']\n",
      "\n",
      "‚ùå PROBLEMS WITH STATIC APPROACH:\n",
      "  ‚Ä¢ Manual maintenance required for new technologies\n",
      "  ‚Ä¢ Hardcoded relationships may not reflect real data patterns\n",
      "  ‚Ä¢ No automatic discovery of emerging technology stacks\n",
      "  ‚Ä¢ Fixed categories don't adapt to industry changes\n"
     ]
    }
   ],
   "source": [
    "# Current Static Technology Ecosystems (to be replaced)\n",
    "current_tech_ecosystems = {\n",
    "    \"python\": [\n",
    "        \"python\", \"django\", \"flask\", \"pandas\", \"numpy\", \"scipy\", \"tensorflow\", \n",
    "        \"pytorch\", \"sklearn\", \"fastapi\", \"celery\", \"redis\", \"postgresql\", \n",
    "        \"mongodb\", \"aws\", \"docker\", \"kubernetes\", \"git\", \"linux\", \"sql\", \n",
    "        \"rest\", \"api\", \"ml\", \"ai\", \"data\", \"jupyter\", \"anaconda\", \"pip\", \n",
    "        \"virtualenv\", \"pytest\", \"matplotlib\", \"seaborn\"\n",
    "    ],\n",
    "    \"java\": [\n",
    "        \"java\", \"spring\", \"springboot\", \"hibernate\", \"maven\", \"gradle\", \n",
    "        \"junit\", \"tomcat\", \"mysql\", \"postgresql\", \"oracle\", \"aws\", \"docker\", \n",
    "        \"kubernetes\", \"git\", \"linux\", \"sql\", \"rest\", \"api\", \"microservices\", \n",
    "        \"kafka\", \"elasticsearch\", \"jenkins\"\n",
    "    ],\n",
    "    \"javascript\": [\n",
    "        \"javascript\", \"js\", \"node\", \"nodejs\", \"react\", \"angular\", \"vue\", \n",
    "        \"express\", \"nestjs\", \"typescript\", \"html\", \"css\", \"sass\", \"scss\", \n",
    "        \"mongodb\", \"mysql\", \"postgresql\", \"aws\", \"docker\", \"git\", \"npm\", \n",
    "        \"yarn\", \"webpack\", \"babel\", \"rest\", \"api\", \"graphql\"\n",
    "    ],\n",
    "    \"react\": [\n",
    "        \"react\", \"javascript\", \"js\", \"jsx\", \"tsx\", \"redux\", \"typescript\", \n",
    "        \"node\", \"express\", \"html\", \"css\", \"sass\", \"scss\", \"webpack\", \n",
    "        \"babel\", \"npm\", \"yarn\", \"git\", \"rest\", \"api\", \"hooks\", \"nextjs\"\n",
    "    ],\n",
    "    \"data\": [\n",
    "        \"python\", \"sql\", \"pandas\", \"numpy\", \"matplotlib\", \"seaborn\", \n",
    "        \"tensorflow\", \"pytorch\", \"sklearn\", \"jupyter\", \"tableau\", \n",
    "        \"powerbi\", \"excel\", \"r\", \"spark\", \"hadoop\", \"aws\", \"azure\", \n",
    "        \"gcp\", \"mongodb\", \"postgresql\", \"mysql\", \"bigquery\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Current hardcoded important technologies list\n",
    "current_important_tech = [\n",
    "    \"python\", \"java\", \"javascript\", \"react\", \"angular\", \"vue\", \"node\", \n",
    "    \"express\", \"django\", \"flask\", \"spring\", \"html\", \"css\", \"sql\", \n",
    "    \"mongodb\", \"mysql\", \"postgresql\", \"redis\", \"aws\", \"azure\", \"gcp\", \n",
    "    \"docker\", \"kubernetes\", \"git\", \"api\", \"rest\", \"graphql\", \"tensorflow\", \n",
    "    \"pytorch\", \"pandas\", \"numpy\", \"typescript\"\n",
    "]\n",
    "\n",
    "# Analyze static data\n",
    "print(\"üîç CURRENT STATIC DATA ANALYSIS:\")\n",
    "print(f\"üìà Total Ecosystems: {len(current_tech_ecosystems)}\")\n",
    "print(f\"üìä Total Unique Technologies in Ecosystems: {len(set().union(*current_tech_ecosystems.values()))}\")\n",
    "print(f\"‚≠ê Important Technologies Count: {len(current_important_tech)}\")\n",
    "\n",
    "# Find overlaps between ecosystems\n",
    "ecosystem_overlaps = {}\n",
    "for eco1, techs1 in current_tech_ecosystems.items():\n",
    "    for eco2, techs2 in current_tech_ecosystems.items():\n",
    "        if eco1 != eco2:\n",
    "            overlap = set(techs1) & set(techs2)\n",
    "            if overlap:\n",
    "                ecosystem_overlaps[f\"{eco1} ‚Üî {eco2}\"] = list(overlap)\n",
    "\n",
    "print(f\"\\nüîÑ Cross-Ecosystem Overlaps: {len(ecosystem_overlaps)}\")\n",
    "for pair, overlaps in list(ecosystem_overlaps.items())[:3]:\n",
    "    print(f\"  ‚Ä¢ {pair}: {overlaps[:5]}{'...' if len(overlaps) > 5 else ''}\")\n",
    "\n",
    "print(f\"\\n‚ùå PROBLEMS WITH STATIC APPROACH:\")\n",
    "print(\"  ‚Ä¢ Manual maintenance required for new technologies\")\n",
    "print(\"  ‚Ä¢ Hardcoded relationships may not reflect real data patterns\")\n",
    "print(\"  ‚Ä¢ No automatic discovery of emerging technology stacks\")\n",
    "print(\"  ‚Ä¢ Fixed categories don't adapt to industry changes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e32e2ff",
   "metadata": {},
   "source": [
    "## üîÑ Dynamic Technology Relationship Discovery\n",
    "\n",
    "Now let's build algorithms to dynamically discover technology relationships from database content, replacing the static mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff6611dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî® Building dynamic technology relationships...\n",
      "‚úÖ Built relationships for 0 technologies\n",
      "\n",
      "üìä DYNAMIC RELATIONSHIP SAMPLE:\n",
      "\n",
      "‚ú® ADVANTAGES OF DYNAMIC APPROACH:\n",
      "  ‚Ä¢ Automatically discovers new technology relationships\n",
      "  ‚Ä¢ Reflects actual data patterns in the database\n",
      "  ‚Ä¢ Self-updating as new resumes are added\n",
      "  ‚Ä¢ No manual maintenance required\n"
     ]
    }
   ],
   "source": [
    "def extract_skills_dynamic(raw_data: str) -> List[str]:\n",
    "    \"\"\"Enhanced skill extraction for dynamic analysis\"\"\"\n",
    "    if not raw_data or not isinstance(raw_data, str):\n",
    "        return []\n",
    "\n",
    "    skills = []\n",
    "    delimiters = r\"[,;|/&]|\\sand\\s|\\sor\\s\"\n",
    "    raw_skills = re.split(delimiters, raw_data, flags=re.IGNORECASE)\n",
    "\n",
    "    for skill in raw_skills:\n",
    "        cleaned = re.sub(r\"^[:\\-\\s]+|[:\\-\\s]+$\", \"\", skill)\n",
    "        cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
    "\n",
    "        if (cleaned and len(cleaned) >= 2 and len(cleaned) <= 50 \n",
    "            and not re.match(r\"^\\d+$\", cleaned)\n",
    "            and cleaned.lower() not in {\"others\", \"and\", \"in\", \"of\", \"the\", \"with\", \"using\", \"etc\", \"various\"}):\n",
    "            skills.append(cleaned.lower())  # Normalize to lowercase for analysis\n",
    "\n",
    "    return skills\n",
    "\n",
    "def build_dynamic_technology_relationships(collection) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Build technology relationship matrix from actual database content\n",
    "    This replaces the static tech_ecosystems dictionary\n",
    "    \"\"\"\n",
    "    print(\"üî® Building dynamic technology relationships...\")\n",
    "    \n",
    "    # Get documents with skills data\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$match\": {\n",
    "                \"$or\": [\n",
    "                    {\"skills\": {\"$exists\": True, \"$ne\": [], \"$ne\": None}},\n",
    "                    {\"technical_skills\": {\"$exists\": True, \"$ne\": [], \"$ne\": None}},\n",
    "                    {\"experience.skills\": {\"$exists\": True, \"$ne\": [], \"$ne\": None}}\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        {\"$limit\": 1000},  # Sample for analysis\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"skills\": 1,\n",
    "                \"technical_skills\": 1,\n",
    "                \"experience.skills\": 1,\n",
    "                \"experience.title\": 1,\n",
    "                \"_id\": 0\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    documents = collection.aggregate(pipeline)\n",
    "    \n",
    "    # Build co-occurrence matrix\n",
    "    skill_cooccurrence = defaultdict(lambda: defaultdict(int))\n",
    "    skill_frequency = defaultdict(int)\n",
    "    \n",
    "    for doc in documents:\n",
    "        doc_skills = set()\n",
    "        \n",
    "        # Extract skills from all fields\n",
    "        for field_name in [\"skills\", \"technical_skills\"]:\n",
    "            field_data = doc.get(field_name)\n",
    "            if field_data:\n",
    "                if isinstance(field_data, list):\n",
    "                    for item in field_data:\n",
    "                        if isinstance(item, str):\n",
    "                            doc_skills.update(extract_skills_dynamic(item))\n",
    "                elif isinstance(field_data, str):\n",
    "                    doc_skills.update(extract_skills_dynamic(field_data))\n",
    "        \n",
    "        # Extract from experience\n",
    "        experience_data = doc.get(\"experience\")\n",
    "        if isinstance(experience_data, list):\n",
    "            for exp in experience_data:\n",
    "                if isinstance(exp, dict):\n",
    "                    exp_skills = exp.get(\"skills\")\n",
    "                    if exp_skills:\n",
    "                        if isinstance(exp_skills, list):\n",
    "                            for skill in exp_skills:\n",
    "                                if isinstance(skill, str):\n",
    "                                    doc_skills.update(extract_skills_dynamic(skill))\n",
    "                        elif isinstance(exp_skills, str):\n",
    "                            doc_skills.update(extract_skills_dynamic(exp_skills))\n",
    "                    \n",
    "                    # Extract from job titles\n",
    "                    title = exp.get(\"title\")\n",
    "                    if title and isinstance(title, str):\n",
    "                        title_words = re.findall(r\"\\b[a-zA-Z]{2,}\\b\", title.lower())\n",
    "                        for word in title_words:\n",
    "                            if word not in {\"senior\", \"junior\", \"lead\", \"manager\", \"engineer\", \n",
    "                                          \"developer\", \"analyst\", \"specialist\", \"consultant\"}:\n",
    "                                doc_skills.add(word)\n",
    "        \n",
    "        # Update frequency counts\n",
    "        for skill in doc_skills:\n",
    "            skill_frequency[skill] += 1\n",
    "        \n",
    "        # Build co-occurrence relationships\n",
    "        doc_skills_list = list(doc_skills)\n",
    "        for i, skill1 in enumerate(doc_skills_list):\n",
    "            for j, skill2 in enumerate(doc_skills_list):\n",
    "                if i != j:\n",
    "                    skill_cooccurrence[skill1][skill2] += 1\n",
    "    \n",
    "    # Convert to relationship scores (normalized by frequency)\n",
    "    relationships = {}\n",
    "    for skill, related_skills in skill_cooccurrence.items():\n",
    "        if skill_frequency[skill] >= 3:  # Minimum frequency threshold\n",
    "            relationships[skill] = {}\n",
    "            for related_skill, count in related_skills.items():\n",
    "                if count >= 2:  # Minimum co-occurrence\n",
    "                    # Normalize by frequencies to get relationship strength\n",
    "                    score = count / (skill_frequency[skill] + skill_frequency[related_skill])\n",
    "                    relationships[skill][related_skill] = score\n",
    "    \n",
    "    print(f\"‚úÖ Built relationships for {len(relationships)} technologies\")\n",
    "    return relationships\n",
    "\n",
    "# Test the dynamic relationship building\n",
    "dynamic_relationships = build_dynamic_technology_relationships(collection)\n",
    "\n",
    "print(\"\\nüìä DYNAMIC RELATIONSHIP SAMPLE:\")\n",
    "sample_skills = list(dynamic_relationships.keys())[:3]\n",
    "for skill in sample_skills:\n",
    "    related = dict(sorted(dynamic_relationships[skill].items(), key=lambda x: x[1], reverse=True)[:5])\n",
    "    print(f\"  ‚Ä¢ {skill}: {list(related.keys())}\")\n",
    "\n",
    "print(f\"\\n‚ú® ADVANTAGES OF DYNAMIC APPROACH:\")\n",
    "print(\"  ‚Ä¢ Automatically discovers new technology relationships\")\n",
    "print(\"  ‚Ä¢ Reflects actual data patterns in the database\")\n",
    "print(\"  ‚Ä¢ Self-updating as new resumes are added\")\n",
    "print(\"  ‚Ä¢ No manual maintenance required\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9bef3a",
   "metadata": {},
   "source": [
    "## üß† Dynamic Ecosystem Discovery\n",
    "\n",
    "Build algorithms to automatically identify technology ecosystems based on clustering and semantic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c00483e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Discovering technology ecosystems dynamically...\n",
      "üéØ DISCOVERED ECOSYSTEMS: 0\n",
      "\n",
      "üîç DYNAMIC CONTEXT EXAMPLES:\n",
      "  ‚Ä¢ 'python' ‚Üí []\n",
      "  ‚Ä¢ 'react' ‚Üí []\n",
      "  ‚Ä¢ 'data' ‚Üí []\n",
      "\n",
      "üí° DYNAMIC ECOSYSTEM BENEFITS:\n",
      "  ‚Ä¢ Automatically adapts to new technology trends\n",
      "  ‚Ä¢ Discovers unexpected technology relationships\n",
      "  ‚Ä¢ Scales with database growth\n",
      "  ‚Ä¢ Eliminates manual ecosystem curation\n"
     ]
    }
   ],
   "source": [
    "def discover_technology_ecosystems(relationships: Dict[str, Dict[str, float]], \n",
    "                                min_cluster_size: int = 5) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Automatically discover technology ecosystems using clustering\n",
    "    This replaces hardcoded ecosystem definitions\n",
    "    \"\"\"\n",
    "    print(\"üîç Discovering technology ecosystems dynamically...\")\n",
    "    \n",
    "    # Create adjacency matrix for clustering\n",
    "    all_skills = set(relationships.keys())\n",
    "    for skill_dict in relationships.values():\n",
    "        all_skills.update(skill_dict.keys())\n",
    "    \n",
    "    skill_list = list(all_skills)\n",
    "    skill_to_idx = {skill: idx for idx, skill in enumerate(skill_list)}\n",
    "    \n",
    "    # Build adjacency matrix\n",
    "    n = len(skill_list)\n",
    "    adjacency = np.zeros((n, n))\n",
    "    \n",
    "    for skill1, related in relationships.items():\n",
    "        if skill1 in skill_to_idx:\n",
    "            i = skill_to_idx[skill1]\n",
    "            for skill2, score in related.items():\n",
    "                if skill2 in skill_to_idx:\n",
    "                    j = skill_to_idx[skill2]\n",
    "                    adjacency[i][j] = score\n",
    "    \n",
    "    # Simple clustering based on strong connections\n",
    "    ecosystems = {}\n",
    "    visited = set()\n",
    "    \n",
    "    def find_cluster(start_skill, threshold=0.1):\n",
    "        cluster = {start_skill}\n",
    "        queue = [start_skill]\n",
    "        \n",
    "        while queue:\n",
    "            current = queue.pop(0)\n",
    "            if current in relationships:\n",
    "                for related, score in relationships[current].items():\n",
    "                    if score > threshold and related not in cluster and len(cluster) < 20:\n",
    "                        cluster.add(related)\n",
    "                        if related not in visited:\n",
    "                            queue.append(related)\n",
    "        \n",
    "        return cluster\n",
    "    \n",
    "    ecosystem_id = 0\n",
    "    for skill in skill_list:\n",
    "        if skill not in visited and skill in relationships:\n",
    "            cluster = find_cluster(skill)\n",
    "            if len(cluster) >= min_cluster_size:\n",
    "                # Name ecosystem after most connected skill\n",
    "                skill_connections = {s: len(relationships.get(s, {})) for s in cluster}\n",
    "                ecosystem_name = max(skill_connections.items(), key=lambda x: x[1])[0]\n",
    "                ecosystems[ecosystem_name] = list(cluster)\n",
    "                visited.update(cluster)\n",
    "                ecosystem_id += 1\n",
    "    \n",
    "    return ecosystems\n",
    "\n",
    "def get_dynamic_ecosystem_context(search_term: str, \n",
    "                                relationships: Dict[str, Dict[str, float]],\n",
    "                                ecosystems: Dict[str, List[str]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get contextually relevant technologies for a search term\n",
    "    This replaces the static ecosystem lookup\n",
    "    \"\"\"\n",
    "    search_lower = search_term.lower()\n",
    "    context_skills = set()\n",
    "    \n",
    "    # Direct relationship lookup\n",
    "    if search_lower in relationships:\n",
    "        related = sorted(relationships[search_lower].items(), key=lambda x: x[1], reverse=True)\n",
    "        context_skills.update([skill for skill, score in related[:10] if score > 0.05])\n",
    "    \n",
    "    # Ecosystem-based context\n",
    "    for ecosystem_name, skills in ecosystems.items():\n",
    "        if search_lower in skills or any(search_lower in skill for skill in skills):\n",
    "            context_skills.update(skills[:15])  # Add ecosystem members\n",
    "    \n",
    "    # Partial matching for flexibility\n",
    "    for skill in relationships.keys():\n",
    "        if search_lower in skill or skill in search_lower:\n",
    "            if skill in relationships:\n",
    "                related = sorted(relationships[skill].items(), key=lambda x: x[1], reverse=True)\n",
    "                context_skills.update([s for s, score in related[:5] if score > 0.1])\n",
    "    \n",
    "    return list(context_skills)[:20]\n",
    "\n",
    "# Test ecosystem discovery\n",
    "ecosystems = discover_technology_ecosystems(dynamic_relationships)\n",
    "\n",
    "print(f\"üéØ DISCOVERED ECOSYSTEMS: {len(ecosystems)}\")\n",
    "for name, skills in list(ecosystems.items())[:3]:\n",
    "    print(f\"  ‚Ä¢ {name.upper()} Ecosystem: {skills[:8]}{'...' if len(skills) > 8 else ''}\")\n",
    "\n",
    "# Test dynamic context generation\n",
    "test_terms = [\"python\", \"react\", \"data\"]\n",
    "print(f\"\\nüîç DYNAMIC CONTEXT EXAMPLES:\")\n",
    "for term in test_terms:\n",
    "    context = get_dynamic_ecosystem_context(term, dynamic_relationships, ecosystems)\n",
    "    print(f\"  ‚Ä¢ '{term}' ‚Üí {context[:6]}{'...' if len(context) > 6 else ''}\")\n",
    "\n",
    "print(f\"\\nüí° DYNAMIC ECOSYSTEM BENEFITS:\")\n",
    "print(\"  ‚Ä¢ Automatically adapts to new technology trends\")\n",
    "print(\"  ‚Ä¢ Discovers unexpected technology relationships\")\n",
    "print(\"  ‚Ä¢ Scales with database growth\")\n",
    "print(\"  ‚Ä¢ Eliminates manual ecosystem curation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10462d8d",
   "metadata": {},
   "source": [
    "## ‚ö° Refactored Dynamic Scoring Algorithm\n",
    "\n",
    "Redesign the calculate_relevance_score function to use dynamic data instead of hardcoded lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1021542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TESTING DYNAMIC SCORING:\n",
      "  ‚Ä¢ 'python' ‚Üí 'django': 0.000\n",
      "  ‚Ä¢ 'javascript' ‚Üí 'react': 0.000\n",
      "  ‚Ä¢ 'python' ‚Üí 'the': 0.100\n",
      "\n",
      "üåü DYNAMICALLY DISCOVERED IMPORTANT SKILLS:\n",
      "  Top 10: ['the', 'and', 'with', 'python']\n",
      "\n",
      "‚ú® DYNAMIC SCORING ADVANTAGES:\n",
      "  ‚Ä¢ No hardcoded technology lists to maintain\n",
      "  ‚Ä¢ Adapts to changing technology landscapes\n",
      "  ‚Ä¢ Uses actual data patterns for scoring\n",
      "  ‚Ä¢ Automatically penalizes generic terms\n",
      "  ‚Ä¢ Relationship-aware scoring\n"
     ]
    }
   ],
   "source": [
    "def calculate_dynamic_relevance_score(text: str, prefix: str, \n",
    "                                     relationships: Dict[str, Dict[str, float]],\n",
    "                                     skill_frequencies: Dict[str, int] = None,\n",
    "                                     semantic_score: float = 0.0) -> float:\n",
    "    \"\"\"\n",
    "    NEW: Dynamic relevance scoring without hardcoded technology lists\n",
    "    Replaces the static tech_ecosystems and important_tech dependencies\n",
    "    \"\"\"\n",
    "    if not text or not prefix:\n",
    "        return semantic_score\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    prefix_lower = prefix.lower()\n",
    "    base_score = semantic_score\n",
    "\n",
    "    # Core relevance scoring (unchanged)\n",
    "    if text_lower == prefix_lower:\n",
    "        base_score = max(1.0, semantic_score + 0.5)\n",
    "    elif text_lower.startswith(prefix_lower):\n",
    "        base_score = max(0.9 - (len(text) - len(prefix)) * 0.01, semantic_score + 0.3)\n",
    "    elif prefix_lower in text_lower:\n",
    "        position = text_lower.find(prefix_lower)\n",
    "        position_penalty = position * 0.01\n",
    "        base_score = max(0.7 - position_penalty, semantic_score + 0.2)\n",
    "    elif re.search(r\"\\b\" + re.escape(prefix_lower), text_lower):\n",
    "        base_score = max(0.6, semantic_score + 0.15)\n",
    "    else:\n",
    "        similarity = difflib.SequenceMatcher(None, prefix_lower, text_lower).ratio()\n",
    "        if similarity > 0.7:\n",
    "            base_score = max(0.5 * similarity, semantic_score + 0.1)\n",
    "        else:\n",
    "            words = text_lower.split()\n",
    "            for word in words:\n",
    "                if word.startswith(prefix_lower):\n",
    "                    base_score = max(0.4, semantic_score + 0.1)\n",
    "                    break\n",
    "            else:\n",
    "                if len(prefix) >= 2:\n",
    "                    prefix_chars = list(prefix_lower)\n",
    "                    text_chars = list(text_lower.replace(\" \", \"\"))\n",
    "                    i = 0\n",
    "                    for char in text_chars:\n",
    "                        if i < len(prefix_chars) and char == prefix_chars[i]:\n",
    "                            i += 1\n",
    "                    if i == len(prefix_chars):\n",
    "                        base_score = max(0.3, semantic_score)\n",
    "\n",
    "    # DYNAMIC BONUSES (replacing static lists)\n",
    "    \n",
    "    # 1. Relationship-based bonus\n",
    "    relationship_bonus = 0.0\n",
    "    if prefix_lower in relationships and text_lower in relationships[prefix_lower]:\n",
    "        relationship_strength = relationships[prefix_lower][text_lower]\n",
    "        relationship_bonus = min(relationship_strength * 2, 0.3)\n",
    "    \n",
    "    # Reverse relationship check\n",
    "    if text_lower in relationships and prefix_lower in relationships[text_lower]:\n",
    "        reverse_strength = relationships[text_lower][prefix_lower]\n",
    "        relationship_bonus = max(relationship_bonus, min(reverse_strength * 2, 0.3))\n",
    "    \n",
    "    # 2. Frequency-based importance (replaces hardcoded important_tech)\n",
    "    frequency_bonus = 0.0\n",
    "    if skill_frequencies and text_lower in skill_frequencies:\n",
    "        # Normalize frequency to bonus (0.0 to 0.2 range)\n",
    "        max_freq = max(skill_frequencies.values()) if skill_frequencies else 1\n",
    "        frequency_ratio = skill_frequencies[text_lower] / max_freq\n",
    "        frequency_bonus = min(frequency_ratio * 0.2, 0.2)\n",
    "    \n",
    "    # 3. Ecosystem co-occurrence bonus\n",
    "    ecosystem_bonus = 0.0\n",
    "    if relationships:\n",
    "        # Count how many times this skill appears with the prefix in relationships\n",
    "        cooccurrence_count = 0\n",
    "        for skill, related in relationships.items():\n",
    "            if prefix_lower in skill and text_lower in related:\n",
    "                cooccurrence_count += 1\n",
    "            if text_lower in skill and prefix_lower in related:\n",
    "                cooccurrence_count += 1\n",
    "        \n",
    "        if cooccurrence_count > 0:\n",
    "            ecosystem_bonus = min(cooccurrence_count * 0.05, 0.15)\n",
    "    \n",
    "    # 4. Dynamic penalty for generic terms (data-driven)\n",
    "    generic_penalty = 0.0\n",
    "    if skill_frequencies and text_lower in skill_frequencies:\n",
    "        # If a term appears too frequently, it might be generic\n",
    "        total_skills = len(skill_frequencies)\n",
    "        if skill_frequencies[text_lower] > total_skills * 0.1:  # Appears in >10% of records\n",
    "            generic_penalty = -0.1\n",
    "    \n",
    "    # Apply dynamic bonuses\n",
    "    final_score = base_score + relationship_bonus + frequency_bonus + ecosystem_bonus + generic_penalty\n",
    "    \n",
    "    return max(0.0, min(2.0, final_score))\n",
    "\n",
    "def get_dynamic_skill_importance(relationships: Dict[str, Dict[str, float]], \n",
    "                               skill_frequencies: Dict[str, int]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Dynamically determine important skills based on data patterns\n",
    "    Replaces the hardcoded important_tech list\n",
    "    \"\"\"\n",
    "    importance_scores = {}\n",
    "    \n",
    "    for skill in skill_frequencies:\n",
    "        score = 0.0\n",
    "        \n",
    "        # Frequency component (normalized)\n",
    "        if skill_frequencies:\n",
    "            max_freq = max(skill_frequencies.values())\n",
    "            freq_score = skill_frequencies[skill] / max_freq\n",
    "            score += freq_score * 0.4\n",
    "        \n",
    "        # Relationship centrality (how connected is this skill)\n",
    "        if skill in relationships:\n",
    "            centrality = len(relationships[skill])\n",
    "            max_centrality = max(len(related) for related in relationships.values()) if relationships else 1\n",
    "            centrality_score = centrality / max_centrality\n",
    "            score += centrality_score * 0.6\n",
    "        \n",
    "        importance_scores[skill] = score\n",
    "    \n",
    "    # Return top important skills\n",
    "    sorted_skills = sorted(importance_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [skill for skill, score in sorted_skills if score > 0.3]\n",
    "\n",
    "# Test the dynamic scoring\n",
    "sample_frequencies = {\n",
    "    \"python\": 150, \"javascript\": 120, \"java\": 100, \"react\": 80, \n",
    "    \"sql\": 90, \"html\": 70, \"css\": 65, \"docker\": 45, \"git\": 85,\n",
    "    \"the\": 200, \"and\": 180, \"with\": 160  # Generic terms with high frequency\n",
    "}\n",
    "\n",
    "print(\"üß™ TESTING DYNAMIC SCORING:\")\n",
    "test_cases = [(\"python\", \"django\"), (\"javascript\", \"react\"), (\"python\", \"the\")]\n",
    "\n",
    "for prefix, text in test_cases:\n",
    "    old_score = 0.5  # Simulated old static score\n",
    "    new_score = calculate_dynamic_relevance_score(\n",
    "        text, prefix, dynamic_relationships, sample_frequencies\n",
    "    )\n",
    "    print(f\"  ‚Ä¢ '{prefix}' ‚Üí '{text}': {new_score:.3f}\")\n",
    "\n",
    "# Get dynamic important skills\n",
    "important_skills = get_dynamic_skill_importance(dynamic_relationships, sample_frequencies)\n",
    "print(f\"\\nüåü DYNAMICALLY DISCOVERED IMPORTANT SKILLS:\")\n",
    "print(f\"  Top 10: {important_skills[:10]}\")\n",
    "\n",
    "print(f\"\\n‚ú® DYNAMIC SCORING ADVANTAGES:\")\n",
    "print(\"  ‚Ä¢ No hardcoded technology lists to maintain\")\n",
    "print(\"  ‚Ä¢ Adapts to changing technology landscapes\")\n",
    "print(\"  ‚Ä¢ Uses actual data patterns for scoring\")\n",
    "print(\"  ‚Ä¢ Automatically penalizes generic terms\")\n",
    "print(\"  ‚Ä¢ Relationship-aware scoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573b648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def refactored_get_skills_from_titles(job_titles: List[str], \n",
    "                                          db_collection,\n",
    "                                          relationships: Dict[str, Dict[str, float]] = None,\n",
    "                                          min_relevance: float = 0.3) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    REFACTORED: Skills extraction without hardcoded limits or static data\n",
    "    Dynamic result sizing based on relevance quality\n",
    "    \"\"\"\n",
    "    if not job_titles:\n",
    "        return {\"error\": \"No job titles provided\"}\n",
    "    \n",
    "    # Build query for all job titles\n",
    "    title_patterns = [{\"$regex\": title.strip(), \"$options\": \"i\"} for title in job_titles]\n",
    "    \n",
    "    pipeline = [\n",
    "        {\"$match\": {\"$or\": [{\"job_title\": {\"$in\": title_patterns}}] + \n",
    "                           [{\"job_title\": pattern} for pattern in title_patterns]}},\n",
    "        {\"$group\": {\n",
    "            \"_id\": None,\n",
    "            \"all_skills\": {\"$push\": \"$skills\"},\n",
    "            \"all_job_titles\": {\"$push\": \"$job_title\"},\n",
    "            \"total_matches\": {\"$sum\": 1}\n",
    "        }}\n",
    "    ]\n",
    "    \n",
    "    result = await db_collection.aggregate(pipeline).to_list(1)\n",
    "    if not result:\n",
    "        return {\"skills\": [], \"metadata\": {\"matches\": 0, \"message\": \"No matching job titles found\"}}\n",
    "    \n",
    "    # Flatten and analyze skills\n",
    "    all_skills = []\n",
    "    for skill_list in result[0][\"all_skills\"]:\n",
    "        if isinstance(skill_list, list):\n",
    "            all_skills.extend(skill_list)\n",
    "    \n",
    "    # Count skill frequencies\n",
    "    skill_counts = Counter(skill.lower().strip() for skill in all_skills if skill and skill.strip())\n",
    "    \n",
    "    # Calculate dynamic relevance scores\n",
    "    scored_skills = []\n",
    "    for skill, count in skill_counts.items():\n",
    "        # Base frequency score\n",
    "        frequency_score = min(count / len(job_titles), 1.0)\n",
    "        \n",
    "        # Relationship bonus\n",
    "        relationship_score = 0.0\n",
    "        if relationships:\n",
    "            for title in job_titles:\n",
    "                title_lower = title.lower()\n",
    "                if title_lower in relationships and skill in relationships[title_lower]:\n",
    "                    relationship_score += relationships[title_lower][skill]\n",
    "        \n",
    "        # Final relevance score\n",
    "        relevance_score = frequency_score + (relationship_score / len(job_titles))\n",
    "        \n",
    "        if relevance_score >= min_relevance:\n",
    "            scored_skills.append({\n",
    "                \"skill\": skill.title(),\n",
    "                \"frequency\": count,\n",
    "                \"relevance_score\": round(relevance_score, 3),\n",
    "                \"category\": determine_skill_category(skill, relationships)\n",
    "            })\n",
    "    \n",
    "    # Sort by relevance and frequency\n",
    "    scored_skills.sort(key=lambda x: (x[\"relevance_score\"], x[\"frequency\"]), reverse=True)\n",
    "    \n",
    "    # DYNAMIC RESULT SIZING - no hardcoded limits!\n",
    "    # Include all skills above minimum relevance threshold\n",
    "    high_relevance_skills = [s for s in scored_skills if s[\"relevance_score\"] >= 0.7]\n",
    "    medium_relevance_skills = [s for s in scored_skills if 0.4 <= s[\"relevance_score\"] < 0.7]\n",
    "    low_relevance_skills = [s for s in scored_skills if min_relevance <= s[\"relevance_score\"] < 0.4]\n",
    "    \n",
    "    # Adaptive result sizing based on quality\n",
    "    if len(high_relevance_skills) >= 20:\n",
    "        final_skills = high_relevance_skills[:50]  # Cap only for very high-quality results\n",
    "    elif len(high_relevance_skills) >= 10:\n",
    "        final_skills = high_relevance_skills + medium_relevance_skills[:30]\n",
    "    else:\n",
    "        final_skills = scored_skills  # Return all qualifying skills\n",
    "    \n",
    "    return {\n",
    "        \"skills\": final_skills,\n",
    "        \"metadata\": {\n",
    "            \"total_job_matches\": result[0][\"total_matches\"],\n",
    "            \"unique_skills_found\": len(skill_counts),\n",
    "            \"skills_returned\": len(final_skills),\n",
    "            \"quality_distribution\": {\n",
    "                \"high_relevance\": len(high_relevance_skills),\n",
    "                \"medium_relevance\": len(medium_relevance_skills),\n",
    "                \"low_relevance\": len(low_relevance_skills)\n",
    "            },\n",
    "            \"dynamic_sizing\": True,\n",
    "            \"min_relevance_threshold\": min_relevance\n",
    "        }\n",
    "    }\n",
    "\n",
    "def determine_skill_category(skill: str, relationships: Dict = None) -> str:\n",
    "    \"\"\"\n",
    "    Dynamically categorize skills based on patterns and relationships\n",
    "    No hardcoded technology categories\n",
    "    \"\"\"\n",
    "    skill_lower = skill.lower()\n",
    "    \n",
    "    # Pattern-based categorization\n",
    "    programming_patterns = [\"python\", \"java\", \"javascript\", \"c++\", \"c#\", \".net\", \"php\", \"ruby\"]\n",
    "    database_patterns = [\"sql\", \"mysql\", \"postgresql\", \"mongodb\", \"oracle\", \"database\"]\n",
    "    web_patterns = [\"html\", \"css\", \"react\", \"angular\", \"vue\", \"bootstrap\", \"jquery\"]\n",
    "    cloud_patterns = [\"aws\", \"azure\", \"gcp\", \"cloud\", \"docker\", \"kubernetes\"]\n",
    "    \n",
    "    if any(pattern in skill_lower for pattern in programming_patterns):\n",
    "        return \"Programming Language\"\n",
    "    elif any(pattern in skill_lower for pattern in database_patterns):\n",
    "        return \"Database\"\n",
    "    elif any(pattern in skill_lower for pattern in web_patterns):\n",
    "        return \"Web Technology\"\n",
    "    elif any(pattern in skill_lower for pattern in cloud_patterns):\n",
    "        return \"Cloud/Infrastructure\"\n",
    "    \n",
    "    # Relationship-based categorization\n",
    "    if relationships and skill_lower in relationships:\n",
    "        related_skills = list(relationships[skill_lower].keys())\n",
    "        if any(\"python\" in rel or \"java\" in rel for rel in related_skills):\n",
    "            return \"Programming Language\"\n",
    "        elif any(\"database\" in rel or \"sql\" in rel for rel in related_skills):\n",
    "            return \"Database\"\n",
    "    \n",
    "    return \"General Skill\"\n",
    "\n",
    "print(\"üîÑ REFACTORED API FEATURES:\")\n",
    "print(\"  ‚úÖ No hardcoded limits (dynamic result sizing)\")\n",
    "print(\"  ‚úÖ No static technology lists\")\n",
    "print(\"  ‚úÖ Quality-based adaptive results\")\n",
    "print(\"  ‚úÖ Relationship-aware categorization\")\n",
    "print(\"  ‚úÖ Configurable relevance thresholds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620de4b3",
   "metadata": {},
   "source": [
    "## üß™ Implementation Testing & Validation\n",
    "\n",
    "Before applying the refactored code, let's test the new dynamic approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a0b0b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ TESTING DYNAMIC VS STATIC SCORING:\n",
      "==================================================\n",
      "üî¥ Different High relationship expected\n",
      "    'python' ‚Üí 'django'\n",
      "    Static: 0.850 | Dynamic: 0.000 | Œî: -0.850\n",
      "\n",
      "üî¥ Different Framework relationship\n",
      "    'java' ‚Üí 'spring'\n",
      "    Static: 0.850 | Dynamic: 0.000 | Œî: -0.850\n",
      "\n",
      "üî¥ Different Library relationship\n",
      "    'javascript' ‚Üí 'react'\n",
      "    Static: 0.850 | Dynamic: 0.000 | Œî: -0.850\n",
      "\n",
      "üî¥ Different Should penalize generic term\n",
      "    'python' ‚Üí 'the'\n",
      "    Static: 0.850 | Dynamic: 0.100 | Œî: -0.750\n",
      "\n",
      "üî¥ Different Domain relationship\n",
      "    'web' ‚Üí 'html'\n",
      "    Static: 0.850 | Dynamic: 0.000 | Œî: -0.850\n",
      "\n",
      "üî¥ Different Partial match + context\n",
      "    'machine' ‚Üí 'tensorflow'\n",
      "    Static: 0.850 | Dynamic: 0.000 | Œî: -0.850\n",
      "\n",
      "üìä PERFORMANCE COMPARISON:\n",
      "  ‚Ä¢ Static approach: Fixed bonuses, no adaptation\n",
      "  ‚Ä¢ Dynamic approach: Data-driven, relationship-aware\n",
      "  ‚Ä¢ Key improvement: Contextual relevance based on actual usage patterns\n"
     ]
    }
   ],
   "source": [
    "# Test dynamic scoring vs static scoring\n",
    "print(\"üî¨ TESTING DYNAMIC VS STATIC SCORING:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_scenarios = [\n",
    "    (\"python\", \"django\", \"High relationship expected\"),\n",
    "    (\"java\", \"spring\", \"Framework relationship\"),\n",
    "    (\"javascript\", \"react\", \"Library relationship\"),\n",
    "    (\"python\", \"the\", \"Should penalize generic term\"),\n",
    "    (\"web\", \"html\", \"Domain relationship\"),\n",
    "    (\"machine\", \"tensorflow\", \"Partial match + context\")\n",
    "]\n",
    "\n",
    "# Simulate current static scoring issues\n",
    "static_tech_bonus = 0.2  # Hardcoded bonus for \"important\" tech\n",
    "static_ecosystem_bonus = 0.15  # Hardcoded ecosystem bonus\n",
    "\n",
    "for prefix, text, description in test_scenarios:\n",
    "    # Old static approach simulation\n",
    "    static_score = 0.5 + static_tech_bonus + static_ecosystem_bonus\n",
    "    \n",
    "    # New dynamic approach\n",
    "    dynamic_score = calculate_dynamic_relevance_score(\n",
    "        text, prefix, dynamic_relationships, sample_frequencies\n",
    "    )\n",
    "    \n",
    "    improvement = dynamic_score - static_score\n",
    "    status = \"üü¢ Better\" if improvement > 0 else \"üî¥ Different\" if improvement < -0.1 else \"üü° Similar\"\n",
    "    \n",
    "    print(f\"{status} {description}\")\n",
    "    print(f\"    '{prefix}' ‚Üí '{text}'\")\n",
    "    print(f\"    Static: {static_score:.3f} | Dynamic: {dynamic_score:.3f} | Œî: {improvement:+.3f}\")\n",
    "    print()\n",
    "\n",
    "print(\"üìä PERFORMANCE COMPARISON:\")\n",
    "print(\"  ‚Ä¢ Static approach: Fixed bonuses, no adaptation\")\n",
    "print(\"  ‚Ä¢ Dynamic approach: Data-driven, relationship-aware\")\n",
    "print(\"  ‚Ä¢ Key improvement: Contextual relevance based on actual usage patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f0303d",
   "metadata": {},
   "source": [
    "## üöÄ Implementation Roadmap\n",
    "\n",
    "**Phase 1: Remove Static Data Structures**\n",
    "1. Remove `tech_ecosystems` dictionary (250+ technologies)\n",
    "2. Remove `important_tech` list (35+ technologies)  \n",
    "3. Remove hardcoded limits in skills API\n",
    "\n",
    "**Phase 2: Implement Dynamic Scoring**\n",
    "1. Replace `calculate_relevance_score()` function\n",
    "2. Add dynamic skill relationship building\n",
    "3. Add frequency-based importance calculation\n",
    "\n",
    "**Phase 3: Update API Endpoints**\n",
    "1. Modify `/skills_by_titles/` endpoint for dynamic results\n",
    "2. Update autocomplete endpoints with new scoring\n",
    "3. Remove static technology ecosystem references\n",
    "\n",
    "**Phase 4: Testing & Validation**\n",
    "1. Compare performance with previous static approach\n",
    "2. Validate result quality and relevance\n",
    "3. Monitor API response times and accuracy\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to implement the refactored code!** üéØ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
